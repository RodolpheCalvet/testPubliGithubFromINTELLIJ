

Commandes SPARK pour les furets

Lancer le spark shell :

cd vers /spark......hadoop2.7/bin
puis
./spark-shell

puis configurer le spark shell pour URL atlas et dependency Mongospark (connector_2.12:3.0.0)
MEF PAS DE WHITESPACE entre les packages !!!!!
./spark-shell --conf "spark.mongodb.input.uri=mongodb+srv://admin:admin@cluster0.ldu6v.mongodb.net/DBtest.COLLtest" --conf "spark.mongodb.output.uri=mongodb+srv://admin:admin@cluster0.ldu6v.mongodb.net/DBtest.COLLtest" --packages org.mongodb.scala:mongo-scala-driver_2.12:4.1.1,org.mongodb.spark:mongo-spark-connector_2.12:3.0.0

puis copier collers aavec:
:paste
CTRL V
enter ou ;
fin CTLD (attention CTRL C kill le sparkshell)

Note
Enable MongoDB Connector specific functions and implicits for the SparkSession and RDD (Resilient Distributed Dataset) by importing the following package in the Spark shell:
( pour import com.mongodb.spark._)

//////////////////////////  A RECOLLER A CHAQUE NOUVEAU SHELL //////////////////////////////////////////

import com.mongodb.client.{MongoClient, MongoDatabase}
import org.apache.spark.SparkConf
import org.apache.spark.sql.SparkSession
import com.mongodb.spark.MongoSpark
import org.bson.Document
import com.mongodb.spark._
import com.mongodb.spark.config.ReadConfig
import org.mongodb.scala.bson.codecs.Macros
import org.bson.codecs.configuration.CodecRegistries.{fromProviders, fromRegistries}
import org.mongodb.scala.MongoClient.DEFAULT_CODEC_REGISTRY


val conf = new SparkConf().setAll(Map(
      "spark.master" -> "local",
      "spark.scheduler.mode" -> "FIFO",
      "spark.speculation" -> "false",
      "spark.reducer.maxSizeInFlight" -> "48m",
      "spark.serializer" -> "org.apache.spark.serializer.KryoSerializer",
      "spark.kryoserializer.buffer.max" -> "1g",
      "spark.shuffle.file.buffer" -> "32k",
      "spark.default.parallelism" -> "12",
      "spark.sql.shuffle.partitions" -> "12",
      "spark.mongodb.input.uri" -> "mongodb+srv://admin:admin@cluster0.ldu6v.mongodb.net/DBtest.COLLtest",
      "spark.mongodb.output.uri" -> "mongodb+srv://admin:admin@cluster0.ldu6v.mongodb.net/DBtest.COLLtest"
    ))


val spark = SparkSession.builder()
  .master("local")
  .appName("MongoSparkConnectorIntro")
  .config(conf)
  .getOrCreate()

val df = MongoSpark.load(spark)
df.show(2,truncate=62,vertical=true)

///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

conf.getAll //redonne  les paramas
sc.setLogLevel("ERROR")

//Puis

df.show()
//Fancy print : garde les 62 colonnes et comme c'est trop large on passe en  display vertical. Affichage de 2rows
df.show(2,truncate=62,vertical=true)
//ou encore très bien:
df.take(5).foreach(println)

//Explication : la fonction take d'un RDD prend en paramètre le nombre d'éléments du RDD qu'on veut garder (les n premiers éléments) et renvoie un Array, qui est une structure de données de base en Scala, contenant ces n éléments. Comme mentionné dans la doc, un Array possède la fonction foreach qui permet d'appliquer à chaque élément n'importe quelle fonction. On choisit ici d'utiliser la fonction println qui permet d'afficher une valeur dans le terminal (c'est l'équivalent du print en python). Les 5 lignes de l'output correspondent donc aux 5 premières lignes du fichier.

println(s"Nombre de lignes : ${df.count}")

println(s"Nombre de colonnes : ${df.columns.length}")

df.printSchema()

df
df.count
->2665
df.filter(df("SOURCEURL").contains("covid")).count
->122
ou
df.filter(df("SOURCEURL").like("%COVID%")).count

PIPELINES
https://docs.mongodb.com/spark-connector/current/scala/aggregation
Aggregation
Pass an aggregation pipeline to a MongoRDD instance to filter data and perform aggregations in MongoDB before passing documents to Spark.
The following example uses an aggregation pipeline to perform the same filter operation as the example above; filter all documents where the test field has a value greater than 5:

 Si ss sparkseesion en param, load donne un dataframe normal
val rdd = MongoSpark.load(sc)
rdd.count
->2665
(Créer un MongoRDD : se fait avec sc en param.)
Mais avec ce MongoRDD on a eu une erreur:
val aggregatedRdd = rdd.withPipeline(Seq(Document.parse("{ $match: { EventRootCode  : { $gt : 3 } } }")))
println(aggregatedRdd.count)
donne une erreur JRE...

Mais!
Les pipelines sont faits under the hood avec les dataframes normaux cf doc! 
donc rapidité OK en  pipeline avec commandes du type:

filter
df.filter($"foo".contains("bar"))   rappel $"foo" ==== df(foo)

df.filter(df("Actor2Type1Code") ).show()
ou
df.filter(df("SOURCEURL").like("%covid%")).count
ou
df.filter(df("SOURCEURL").like("%covid%")).select(df("_id"), df("SOURCEURL")).show(5)  //2e select pour select les colonnes affichées
ou
df.filter(df("SOURCEURL").like("%covid%")).select(df("_id"), df("ActionGeo_CountryCode")).groupBy("ActionGeo_CountryCode").count().show(10)














